{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of online reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##a Parse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(file_path):\n",
    "    res = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            now = line.strip().split('\\t')\n",
    "            if len(now) == 2:\n",
    "                res.append({'sentence' : now[0], 'label' : int(now[1])})\n",
    "                 \n",
    "    return pd.DataFrame(res)\n",
    "    \n",
    "amazon_data = read_data('./sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "imdb_data = read_data('./sentiment labelled sentences/imdb_labelled.txt')\n",
    "yelp_data = read_data('./sentiment labelled sentences/yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                           sentence\n",
      "0      0  So there is no way for me to plug it in here i...\n",
      "1      1                        Good case, Excellent value.\n",
      "2      1                             Great for the jawbone.\n",
      "3      0  Tied to charger for conversations lasting more...\n",
      "4      1                                  The mic is great.\n",
      "label 0 count: 500\n",
      "label 1 count: 500\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "print amazon_data.head()\n",
    "def label_info(data):\n",
    "    l_0, l_1 = len(data[data.label == 0]), len(data[data.label == 1])\n",
    "    print \"label 0 count: \" + str(l_0)\n",
    "    print \"label 1 count: \" + str(l_1)\n",
    "    print \"ratio: \" + str((l_0 * 1.0 / l_1))\n",
    "    \n",
    "label_info(amazon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 count: 500\n",
      "label 1 count: 500\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "label_info(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 count: 500\n",
      "label 1 count: 500\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "label_info(yelp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all three datasets' labels are well balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    So there is no way for me to plug it in here i...\n",
      "1                          Good case, Excellent value.\n",
      "2                               Great for the jawbone.\n",
      "3    Tied to charger for conversations lasting more...\n",
      "4                                    The mic is great.\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print amazon_data['sentence'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences contain both uppercase letters and lowercase letter, we need to convert uppcase letters to lowercase because 'Good' has the same meaning as 'good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase(sentence):\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation also helps denoising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "punct = re.compile(r'([^A-Za-z0-9 ])')\n",
    "def remove_punc(sentence):\n",
    "    return punct.sub(\"\", sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization is also necessary, because 'has' and 'have' have same meaning. Here I am using nltk's Porter stemmer to do stem.\n",
    "Removing stop words help us better concentrate on features that 'matters', since words like \"and\", \"or\" make no contribution to classify a sentence.\n",
    "P.S YOU should download corpus by running `nltk.download()` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()\n",
    "    \n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "def stemming_and_remove_stop(sentence):\n",
    "    return ' '.join([stemmer.stem(word) for word in sentence.split() if word not in stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make all the functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    strs = list(data['sentence'])\n",
    "    def preprocess_one_line(sent):\n",
    "        return stemming_and_remove_stop(remove_punc(lowercase(sent)))\n",
    "    return [preprocess_one_line(line) for line in strs]\n",
    "\n",
    "amazon_data['sentence'] = preprocess(amazon_data)\n",
    "yelp_data['sentence'] = preprocess(yelp_data)\n",
    "imdb_data['sentence'] = preprocess(imdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c Split training and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 600\n"
     ]
    }
   ],
   "source": [
    "def extract_train(data):\n",
    "    data_label_1, data_label_0 = data[data.label == 1], data[data.label == 0]\n",
    "    \n",
    "    train_dt_1, test_dt_1 = data_label_1[:400], data_label_1[400:]\n",
    "    train_dt_0, test_dt_0 = data_label_0[:400], data_label_0[400:]\n",
    "    return train_dt_1.append(train_dt_0, ignore_index = True), test_dt_1.append(test_dt_0, ignore_index = True)\n",
    "\n",
    "amz_train, amz_test = extract_train(amazon_data)\n",
    "yp_train, yp_test = extract_train(yelp_data)\n",
    "imdb_train, imdb_test = extract_train(imdb_data)\n",
    "\n",
    "train = amz_train.append(yp_train, ignore_index=True).append(imdb_train, ignore_index=True)\n",
    "test = amz_test.append(yp_test, ignore_index=True).append(imdb_test, ignore_index=True)\n",
    "print len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## d Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict, word_list = {}, []\n",
    "#print train['sentence']\n",
    "\n",
    "def extract_word_pos(data):\n",
    "    pos = 0\n",
    "    for sentence in data['sentence']:\n",
    "        for word in sentence.split(' '):\n",
    "            if not word in word_dict:\n",
    "                word_dict[word] = pos\n",
    "                pos += 1\n",
    "                word_list.append(word)\n",
    "\n",
    "extract_word_pos(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we can't use testing set is that we can not use any information from testing set as it is served as a standalone testing criteria. If we are using word from testing set, then we might overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_feature(data):\n",
    "    rows, cols = len(data['sentence']), len(word_dict)\n",
    "    res = np.zeros((rows,cols), dtype = np.int)\n",
    "    \n",
    "    for index, sentence in enumerate(data['sentence']):\n",
    "        for word in sentence.split(' '):\n",
    "            if word in word_dict:\n",
    "                res[index][word_dict[word]] += 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feat = build_feature(train)\n",
    "test_feat = build_feature(test)\n",
    "train_label, test_label = train['label'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold='nan', linewidth='100')\n",
    "print train_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print train_feat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printing we can see, the feature vectors are very sparse, only a few entries have positive values while most of the entries are filled with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e Postprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def normalize(data, norm = 'l2'):\n",
    "    return preprocessing.normalize(data.astype(np.float64), norm = norm)\n",
    "\n",
    "train_feat = normalize(train_feat)\n",
    "test_feat = normalize(test_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-normalization is useful when converting large data range to small data range while here data's range is quite small, so log-norm is not suitable. Standardizing data is also not useful because the underlying assumption of standardization is the data has a gaussian distribution, which is clearly not true for short texts. L1 normalization only keep the information of relative proportion, for example, after l1 norm, (10, 5) and (2,1) both become (2/3, 1/3), so we lose the information on absolute numbers, which could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f Training set clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_k_centroids(seeds, features, K = 2):\n",
    "    def isEnd(prev_centers, now_centers):\n",
    "        for idx in range(len(prev_centers)):\n",
    "            prev_center, now_center = prev_centers[idx], now_centers[idx]\n",
    "            if not np.allclose(prev_center, now_center,rtol=1e-4,atol=1e-6):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    center_matrices = assign(seeds, features)\n",
    "    \n",
    "    #print center_matrices[0].shape\n",
    "    #print center_matrices[1].shape\n",
    "    prev_centers, now_centers = seeds, [np.mean(matrix, axis=0) for matrix in center_matrices]\n",
    "    \n",
    "    time = 1\n",
    "    while not isEnd(prev_centers, now_centers):\n",
    "        print \"iter \" + str(time)\n",
    "        time += 1\n",
    "        center_matrices = assign(now_centers, features)\n",
    "        prev_centers = now_centers\n",
    "        now_centers = [matrix.mean(0) for matrix in center_matrices]\n",
    "        \n",
    "    labels = np.zeros((features.shape[0],1), dtype=np.int)\n",
    "    \n",
    "    for index, point in enumerate(features):\n",
    "        idx = find_closest_center(now_centers, point)\n",
    "        labels[index, 0] = idx\n",
    "        \n",
    "    return now_centers, labels\n",
    "        \n",
    "def assign(centers, points):\n",
    "    \n",
    "    res = [None for _ in range(len(centers))]\n",
    "    \n",
    "    for point in points:\n",
    "        idx = find_closest_center(centers, point)\n",
    "        if res[idx] is None:\n",
    "            res[idx] = point\n",
    "        else:\n",
    "            res[idx] = np.vstack((res[idx], point))\n",
    "            \n",
    "    return res\n",
    "\n",
    "def find_closest_center(centers, arr):\n",
    "    res_index, dist = -1, float('inf')\n",
    "    \n",
    "    for index,center in enumerate(centers):\n",
    "        now_dist = l2_distance(center, arr)\n",
    "        if now_dist < dist:\n",
    "            res_index, dist = index, now_dist\n",
    "            \n",
    "    return res_index\n",
    "\n",
    "def l2_distance(a, b):\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import k_means\n",
    "np.random.seed(50)\n",
    "\n",
    "i1,i2 = np.random.choice(train_feat.shape[0],2, replace=False)\n",
    "start_points = train_feat[[i1,i2]]\n",
    "centers, labels = find_k_centroids(start_points, train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.42e-02   4.52e-03   8.76e-03   1.58e-03   1.09e-03   4.57e-04   2.57e-04   9.44e-04   2.57e-04   2.52e-03   6.52e-03   9.70e-03   3.58e-03   1.37e-02   1.40e-03   7.13e-03   2.98e-04   2.78e-03   3.64e-03   1.06e-02   1.66e-02   7.28e-04   3.06e-04   2.24e-02   2.82e-03   1.59e-02   1.73e-03   1.18e-02   1.82e-04   2.87e-04   3.06e-03   1.43e-02   8.61e-04   1.39e-03   4.16e-03   3.19e-03   1.60e-03   1.92e-03   1.19e-03   3.17e-03   1.80e-03   2.46e-03   4.11e-04   2.97e-04   3.09e-04   7.84e-03   5.53e-04   3.18e-04   1.70e-03   4.49e-03   1.15e-02   3.42e-04   7.75e-03   1.68e-04   3.36e-03   1.72e-02   1.68e-04   4.19e-03   3.03e-04   2.67e-03   2.41e-03   1.31e-02   1.19e-04   9.60e-04   7.35e-03   1.99e-03   7.02e-03   1.19e-04   1.33e-03   2.63e-03   0.00e+00   2.55e-03   0.00e+00   0.00e+00   7.41e-03   1.84e-03   1.41e-03   7.54e-03   7.24e-03   4.72e-03   2.51e-03   1.81e-03   5.36e-03   1.67e-03   6.97e-03   9.37e-04   1.99e-04   1.34e-04   6.81e-04   2.24e-03   2.01e-03   4.49e-04   1.34e-04   4.16e-04   6.00e-04   1.34e-04   2.69e-04   5.30e-03   1.26e-03   9.37e-04   4.52e-03   6.39e-03   5.85e-03   2.86e-03   3.48e-04   9.66e-03   2.11e-03   0.00e+00   0.00e+00   3.37e-04   1.47e-03   3.85e-03   4.12e-03   2.98e-04   1.07e-03   3.06e-04   3.83e-03   1.39e-02   1.58e-04   2.06e-03   7.53e-03   2.57e-04   3.06e-03   3.83e-04   2.62e-03   6.36e-03   8.35e-04   3.40e-03   3.15e-03   1.42e-03   3.83e-03   1.90e-03   4.22e-03   4.06e-03   1.72e-03   3.80e-04   9.73e-03   6.96e-03   6.87e-04   1.08e-04   1.77e-03   0.00e+00   7.29e-04   3.34e-04   7.38e-04   1.32e-03   2.73e-03   3.99e-03   2.51e-03   3.48e-04   3.16e-03   2.23e-04   1.84e-03   4.97e-04   1.84e-03   1.68e-02   1.82e-04   2.78e-03   2.06e-03   7.36e-03   1.03e-03   7.66e-04   1.58e-04   5.80e-04   1.66e-03   4.68e-03   3.42e-03   4.65e-03   4.12e-03   2.26e-04   4.05e-04   4.26e-04   3.28e-04   1.65e-03   5.57e-03   1.20e-03   9.71e-04   7.12e-04   1.41e-04   8.86e-03   8.51e-04   1.81e-03   1.06e-03   1.15e-04   1.15e-04   2.09e-03   3.92e-03   3.77e-03   3.39e-03   8.80e-04   1.89e-03   3.34e-04   2.01e-03   5.56e-04   1.05e-04   2.10e-04   2.87e-04   1.21e-03   1.05e-04   1.05e-04   2.54e-04   1.05e-04   5.90e-04   1.11e-03   2.60e-04   1.73e-03   1.41e-04   1.13e-03   1.54e-03   9.04e-04   2.70e-04   4.84e-04   3.02e-03   6.63e-04   2.44e-03   2.57e-04   1.90e-03   1.06e-03   8.23e-04   6.35e-03   2.57e-04   2.65e-03   2.97e-03   2.41e-03   1.49e-03   1.48e-03   2.37e-03   1.16e-02   4.88e-04   2.23e-04   7.93e-04   1.59e-03   1.55e-03   2.54e-03   0.00e+00   3.15e-04   9.79e-04   2.57e-04   1.37e-03   4.57e-04   8.72e-04   4.60e-03   1.82e-04   4.69e-04   7.44e-03   1.52e-03   3.91e-04   1.68e-04   1.68e-04   1.68e-04   3.80e-03   4.00e-03   0.00e+00   2.57e-04   1.41e-03   0.00e+00   1.01e-03   2.23e-03   3.82e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   2.43e-04   5.09e-04   1.99e-04   2.32e-03   3.61e-03   1.75e-03   1.43e-03   4.64e-04   8.19e-04   1.33e-03   3.57e-04   2.16e-03   8.37e-04   1.74e-02   1.49e-04   3.37e-04   0.00e+00   0.00e+00   8.39e-04   1.82e-04   0.00e+00   5.25e-04   7.59e-04   1.19e-04   1.19e-04   6.43e-04   9.54e-04   3.48e-03   2.50e-03   5.25e-04   1.99e-04   4.34e-03   6.88e-04   2.34e-03   2.35e-03   3.31e-04   5.38e-04   6.04e-04   7.72e-04   5.85e-04   1.58e-04   2.77e-03   1.23e-02   2.57e-04   1.99e-04   1.99e-04   1.99e-04   1.99e-04   0.00e+00   6.77e-04   1.58e-04   1.58e-04   2.57e-03   1.58e-04   2.86e-04   2.57e-04   7.65e-04   1.03e-03   5.46e-04   1.24e-04   2.14e-03   7.05e-04   1.41e-03   9.78e-04   1.24e-04   5.41e-04   2.23e-04   6.39e-04   5.63e-04   1.55e-03   8.07e-04   1.49e-04   3.89e-03   1.32e-03   6.00e-04   8.37e-04   9.61e-03   6.55e-04   1.06e-03   3.03e-04   2.97e-04   6.97e-04   4.92e-03   4.22e-03   2.57e-04   1.99e-04   4.22e-04   2.93e-03   9.49e-04   5.92e-04   1.82e-04   1.24e-03   1.82e-04   1.82e-04   1.99e-04   3.99e-04   3.91e-04   9.00e-04   5.26e-04   2.23e-04   1.66e-03   6.28e-04   0.00e+00   3.46e-04   3.06e-03   3.89e-03   1.76e-03   2.16e-03   3.97e-03   1.49e-04   1.96e-03   2.23e-04   1.26e-03   1.93e-03   3.64e-04   6.21e-04   8.31e-04   1.99e-04   1.63e-03   0.00e+00   2.94e-04   1.99e-04   0.00e+00   2.89e-04   4.51e-04   3.34e-04   1.29e-04   2.23e-04   5.99e-04   9.00e-04   2.23e-04   3.22e-04   2.23e-04   1.29e-04   1.29e-04   5.03e-04   1.22e-03   1.24e-03   1.01e-03   4.11e-03   1.34e-04   1.34e-04   2.53e-04   5.34e-04   2.57e-04   5.25e-04   9.26e-04   1.58e-04   1.68e-03   1.99e-04   1.97e-03   1.36e-03   3.47e-03   1.09e-03   1.93e-03   8.79e-04   8.59e-04   1.09e-03   0.00e+00   1.11e-04   0.00e+00   1.41e-04   1.41e-04   1.41e-04   1.41e-04   3.91e-04   4.85e-04   1.49e-04   1.49e-04   2.81e-03   1.82e-04   2.57e-04   4.06e-04   9.55e-04   2.77e-04   3.21e-03   4.91e-04   2.06e-03   2.70e-04   2.59e-03   4.59e-04   1.58e-04   5.33e-04   2.81e-04   3.38e-03   4.31e-04   1.42e-03   4.44e-04   1.39e-03   4.39e-04   6.86e-04   1.20e-03   1.07e-03   1.82e-04   4.89e-03   8.54e-04   3.29e-03   1.65e-03   8.53e-04   1.47e-03   3.60e-04   1.07e-03   1.99e-04   1.68e-04   1.68e-04   2.31e-03   0.00e+00   0.00e+00   5.45e-04   1.58e-04   1.58e-04   4.11e-04   1.58e-04   2.29e-03   4.56e-04   2.13e-03   3.50e-04   1.82e-04   1.82e-04   7.36e-04   0.00e+00   1.99e-04   3.96e-03   1.24e-04   8.98e-04   2.92e-04   2.92e-04   2.52e-04   1.82e-04   9.47e-04   6.64e-04   2.66e-03   1.15e-04   1.15e-04   1.15e-04   1.03e-03   1.79e-03   2.70e-03   9.92e-04   3.15e-04   0.00e+00   0.00e+00   6.18e-04   2.23e-04   1.11e-03   1.82e-04   1.82e-04   1.37e-03   4.96e-04   1.78e-03   4.56e-04   1.34e-04   1.34e-04   5.94e-04   0.00e+00   4.79e-03   9.41e-04   1.58e-04   1.58e-04   1.07e-03   0.00e+00   9.55e-04   1.68e-04   6.08e-04   1.68e-04   5.09e-03   3.03e-04   1.68e-04   3.15e-04   1.82e-04   1.05e-03   1.82e-04   2.25e-03   1.24e-04   5.81e-04   1.24e-04   1.24e-04   1.24e-04   6.20e-04   2.97e-04   2.30e-04   2.57e-04   6.21e-04   2.97e-03   2.97e-04   5.67e-04   1.68e-04   1.29e-02   9.55e-04   6.98e-04   2.57e-04   2.57e-04   1.36e-03   1.97e-03   1.19e-04   6.20e-04   8.51e-04   1.19e-04   6.91e-04   3.71e-04   3.71e-04   1.61e-02   1.29e-04   2.97e-04   1.29e-04   1.29e-04   1.14e-03   1.34e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   1.58e-04   1.58e-04   3.70e-03   1.40e-03   1.82e-04   1.94e-03   3.98e-03   1.82e-04   5.73e-04   3.19e-03   1.67e-03   3.15e-04   2.98e-04   6.87e-04   2.73e-04   6.14e-04   7.26e-04   1.60e-03   8.10e-04   1.19e-04   3.92e-03   1.34e-04   3.16e-04   2.70e-03   1.86e-03   6.60e-04   1.82e-04   4.93e-04   1.82e-04   1.15e-03   8.24e-04   1.82e-04   6.96e-04   6.47e-04   1.49e-04   4.64e-04   1.49e-04   1.49e-04   1.02e-04   1.02e-04   1.02e-04   3.48e-03   8.65e-04   9.70e-04   4.26e-04   1.34e-04   6.77e-04   5.30e-04   1.29e-04   4.51e-04   1.29e-04   9.36e-04   1.68e-04   2.49e-04   5.11e-04   1.34e-04   3.13e-03   3.17e-04   1.49e-04   1.24e-03   0.00e+00   3.23e-04   3.90e-04   0.00e+00   3.37e-04   6.67e-03   0.00e+00   0.00e+00   7.39e-04   1.30e-03   6.42e-03   1.96e-03   5.43e-04   1.29e-04   1.29e-04   1.29e-04   4.44e-04   1.29e-04   4.46e-04   6.81e-04   3.30e-04   1.34e-04   5.72e-04   2.34e-03   0.00e+00   3.06e-04   1.15e-04   7.25e-04   1.15e-04   2.27e-03   5.56e-04   1.99e-04   1.99e-04   2.90e-03   2.57e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   1.82e-04   2.57e-04   8.59e-04   1.68e-04   1.05e-03   2.18e-03   1.58e-04   4.95e-04   3.80e-04   2.23e-04   1.77e-03   9.79e-04   4.26e-03   4.83e-03   1.03e-03   2.43e-04   1.19e-04   8.68e-04   1.68e-04   7.50e-04   1.58e-04   1.58e-04   1.58e-04   3.81e-04   3.14e-04   2.32e-03   1.19e-04   2.16e-03   7.95e-04   1.41e-04   4.99e-04   4.05e-04   3.10e-04   1.29e-04   1.29e-04   2.24e-03   2.17e-04   1.49e-04   1.49e-04   9.50e-04   2.63e-03   2.88e-04   4.41e-04   5.37e-04   2.82e-04   3.44e-03   6.65e-04   5.47e-04   1.49e-04   1.49e-04   7.34e-04   1.26e-03   1.49e-04   0.00e+00   4.37e-03   9.73e-04   2.61e-03   1.99e-04   1.68e-04   1.99e-04   2.57e-04   1.68e-04   5.68e-04   4.42e-03   1.68e-04   6.27e-04   3.15e-04   1.04e-03   1.66e-03   1.82e-04   1.82e-04   1.34e-04   0.00e+00   0.00e+00   1.49e-04   0.00e+00   4.39e-04   1.41e-04   1.41e-04   5.75e-04   9.91e-04   2.23e-04   1.49e-04   1.49e-04   1.49e-04   1.19e-03   1.19e-03   5.26e-04   1.34e-04   1.34e-04   3.09e-04   1.41e-04   1.41e-04   6.51e-04   1.68e-03   1.82e-04   5.81e-04   8.89e-04   1.82e-04   3.37e-04   3.09e-04   1.68e-04   1.41e-04   2.50e-03   1.19e-04   1.51e-03   1.78e-03   4.46e-04   8.13e-03   5.13e-03   1.29e-04   1.29e-04   1.29e-04   1.85e-03   2.77e-04   3.51e-04   1.29e-04   4.52e-04   9.40e-04   2.57e-04   1.49e-04   3.04e-03   1.49e-04   4.23e-04   6.81e-04   2.06e-03   4.20e-04   3.15e-04   4.49e-04   5.08e-03   1.03e-03   1.97e-03   1.11e-03   1.82e-04   4.05e-04   4.57e-04   1.07e-03   1.99e-04   1.99e-04   3.16e-04   1.88e-03   4.63e-04   1.24e-04   1.34e-03   5.38e-03   4.34e-04   7.95e-04   1.02e-03   2.97e-04   1.49e-04   2.82e-03   1.17e-02   1.56e-02   3.81e-04   1.29e-03   3.28e-04   3.28e-04   4.69e-04   6.87e-04   6.30e-04   7.49e-04   5.60e-04   1.15e-04   2.34e-04   8.76e-04   9.44e-04   1.15e-04   2.56e-04   1.05e-03   5.11e-04   1.25e-03   3.23e-04   1.99e-04   1.04e-03   1.41e-04   2.49e-04   2.82e-04   9.45e-04   2.23e-04   2.23e-04   8.33e-04   1.09e-03   1.41e-04   1.34e-04   1.58e-04   2.29e-03   6.29e-04   2.23e-04   4.26e-04   7.96e-04   1.68e-04   1.68e-04   2.60e-04   1.49e-04   2.97e-04   3.48e-04   1.18e-03   1.46e-03   1.34e-04   7.24e-04   3.76e-04   1.19e-04   2.77e-04   1.19e-04   2.88e-04   2.23e-04   1.27e-03   2.52e-03   3.57e-04   3.72e-04   2.57e-04   3.15e-04   2.12e-03   3.28e-04   1.99e-04   4.22e-04   1.28e-03   4.96e-04   1.49e-04   1.49e-04   4.76e-03   6.15e-04   1.01e-03   9.28e-04   1.82e-04   2.22e-03   5.89e-04   2.97e-04   1.68e-04   5.63e-04   3.01e-04   1.19e-04   3.01e-04   3.85e-04   1.19e-04   1.19e-04   9.03e-04   1.68e-04   1.68e-04   1.68e-04   2.23e-04   1.52e-03   1.49e-04   1.49e-04   1.49e-04   2.74e-03   1.49e-04   2.23e-04   3.64e-04   3.64e-04   1.49e-04   1.99e-04   1.99e-04   9.97e-04   4.24e-03   1.58e-04   6.83e-04   2.86e-03   1.29e-04   2.33e-03   1.29e-04   7.62e-04   3.57e-04   8.17e-04   2.23e-04   1.13e-03   2.23e-04   5.44e-04   3.39e-04   7.72e-04   4.40e-04   5.54e-04   1.34e-04   1.34e-04   7.25e-04   1.99e-04   1.74e-03   2.01e-03   1.34e-04   5.76e-03   4.84e-04   3.15e-04   7.73e-04   9.79e-04   1.41e-04   4.05e-04   3.19e-03   3.62e-03   2.18e-03   8.63e-04   1.49e-04   1.81e-03   2.23e-04   2.23e-04   7.19e-04   1.13e-03   2.23e-04   2.23e-04   2.57e-04   1.68e-04   6.50e-04   5.16e-04   4.26e-04   3.57e-04   4.57e-04   3.15e-04   1.68e-03   7.91e-04   8.20e-04   1.82e-04   2.00e-03   6.97e-04   1.02e-03   2.57e-04   2.57e-04   6.41e-03   1.58e-04   1.58e-04   1.68e-04   4.41e-04   2.68e-04   1.97e-03   1.49e-04   1.49e-04   2.57e-04   1.82e-04   2.92e-04   1.24e-04   6.55e-04   2.23e-04   3.06e-04   1.55e-03   1.99e-04   1.99e-04   1.99e-04   2.35e-03   4.09e-03   8.06e-04   1.82e-03   2.64e-04   1.49e-04   2.83e-04   1.49e-04   2.57e-04   5.91e-04   1.68e-04   3.09e-04   6.97e-04   2.48e-04   2.63e-04   6.30e-04   5.25e-04   6.61e-04   2.23e-04   0.00e+00   0.00e+00   4.94e-04   7.05e-04   1.68e-04   9.91e-04   7.38e-04   1.55e-03   1.68e-04   5.38e-04   1.49e-04   3.30e-04   1.15e-04   2.56e-04   3.85e-04   1.15e-03   2.57e-04   9.99e-04   1.70e-03   1.68e-04   1.56e-03   1.41e-04   2.34e-03   9.50e-05   1.29e-04   1.29e-04   5.52e-04   4.55e-04   1.05e-04   1.05e-04   3.28e-04   1.82e-04   1.82e-04   1.82e-04   6.83e-04   1.07e-03   1.06e-03   2.86e-04   6.79e-04   1.26e-03   1.58e-04   1.58e-04   1.58e-04   8.30e-04   1.80e-03   1.90e-03   3.03e-04   3.76e-04   8.63e-04   1.34e-04   1.34e-04   4.46e-04   1.01e-03   6.32e-04   1.49e-04   1.49e-04   3.30e-04   9.89e-04   4.31e-03   2.58e-04   1.24e-04   1.24e-04   7.33e-04   1.24e-04   7.05e-04   6.76e-04   2.77e-04   2.23e-04   5.62e-04   2.57e-04   4.96e-04   2.57e-04   3.15e-04   1.58e-04   7.35e-04   1.58e-04   2.23e-04   3.51e-04   5.38e-04   1.68e-04   1.68e-04   5.46e-04   1.82e-04   1.82e-04   1.82e-04   4.67e-04   1.46e-03   3.28e-04   2.11e-04   1.29e-04   4.73e-04   7.41e-04   1.68e-04   1.68e-04   1.68e-04   1.17e-03   1.82e-04   1.41e-04   4.22e-04   1.99e-04   3.07e-04   5.47e-04   1.21e-03   2.82e-04   1.41e-04   4.08e-04   5.85e-04   2.57e-04   2.47e-04   1.24e-04   1.02e-03   1.99e-04   3.18e-04   1.29e-03   3.09e-04   2.60e-04   1.41e-04   1.41e-04   6.33e-04   1.44e-03   3.91e-04   1.42e-03   1.99e-04   9.32e-04   1.99e-04   7.14e-04   1.58e-04   1.58e-04   1.58e-04   3.80e-04   4.46e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   8.79e-04   1.68e-04   1.15e-03   3.15e-04   1.41e-04   1.47e-03   1.99e-04   7.84e-04   5.11e-04   6.09e-04   8.42e-04   9.42e-04   1.29e-04   1.29e-04   1.82e-04   1.32e-03   1.29e-04   1.29e-04   1.68e-04   1.24e-04   2.92e-04   4.80e-04   2.23e-04   9.23e-04   7.20e-04   3.04e-04   1.04e-03   4.46e-04   2.23e-04   2.23e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.41e-04   4.33e-04   1.41e-04   1.41e-04   1.41e-04   2.57e-04   9.45e-04   2.61e-03   1.68e-04   1.04e-03   5.14e-04   7.30e-04   1.49e-04   1.49e-03   1.54e-03   1.15e-04   1.15e-04   1.15e-04   1.15e-04   2.23e-04   2.23e-04   3.96e-03   1.41e-04   3.98e-04   1.41e-04   1.41e-04   8.57e-04   2.75e-04   9.75e-04   1.34e-04   2.92e-04   1.58e-04   1.58e-04   1.58e-04   1.82e-04   3.07e-04   6.39e-04   3.03e-04   3.03e-04   2.53e-04   1.34e-04   1.49e-04   1.49e-04   2.26e-04   2.76e-03   1.58e-04   4.75e-04   2.23e-04   2.63e-04   3.23e-04   5.18e-04   1.24e-04   1.49e-03   3.69e-04   3.76e-04   7.21e-04   5.47e-04   3.01e-04   2.57e-04   2.57e-04   2.57e-04   3.49e-04   1.68e-04   1.82e-04   2.92e-04   1.99e-04   1.68e-04   1.68e-04   3.26e-04   4.87e-04   1.19e-04   2.68e-04   1.19e-04   1.99e-04   1.82e-04   1.82e-04   1.34e-04   1.34e-04   4.04e-04   1.15e-04   1.15e-04   7.72e-04   3.96e-04   1.15e-04   1.15e-04   5.88e-04   2.83e-04   1.34e-04   1.78e-03   1.68e-04   1.58e-04   1.41e-04   1.82e-04   1.99e-04   1.99e-04   1.82e-04   1.64e-03   1.29e-04   2.52e-04   1.29e-04   1.68e-04   1.99e-04   1.99e-04   5.02e-04   3.15e-04   1.68e-04   2.23e-04   1.58e-04   1.41e-04   1.41e-04   1.04e-03   1.41e-04   1.41e-04   1.68e-04   3.17e-04   1.68e-04   1.41e-04   4.57e-04   1.29e-04   3.11e-04   1.29e-04   1.82e-04   1.37e-03   2.92e-04   9.93e-04   1.29e-03   1.82e-04   0.00e+00   1.34e-04   6.68e-04   1.41e-04   1.41e-04   1.41e-04   2.57e-04   1.99e-04   4.15e-04   9.40e-04   1.49e-04   1.49e-04   1.49e-04   3.49e-04   1.59e-03   1.82e-04   1.82e-04   1.82e-04   1.82e-04   1.82e-04   1.67e-02   1.99e-04   1.68e-04   1.68e-04   1.68e-04   9.50e-05   3.31e-04   0.00e+00   5.91e-04   2.67e-03   1.49e-04   2.25e-03   7.29e-04   1.99e-04   8.37e-04   3.47e-03   1.08e-03   8.06e-04   2.23e-04   1.39e-03   8.51e-04   1.40e-03   2.57e-04   1.99e-04   1.48e-03   1.73e-03   1.82e-04   9.51e-04   1.82e-04   9.07e-04   1.30e-03   1.10e-03   2.23e-04   4.92e-04   1.85e-03   7.49e-04   1.80e-03   2.75e-04   2.75e-04   1.34e-04   2.89e-04   1.49e-04   7.25e-04   3.30e-04   5.08e-04   1.51e-03   3.37e-04   2.57e-04   0.00e+00   7.15e-04   1.35e-03   3.39e-04   4.92e-04   4.26e-04   2.57e-04   4.75e-03   4.22e-04   0.00e+00   0.00e+00   0.00e+00   1.82e-04   3.49e-03   5.67e-04   1.41e-04   9.97e-04   8.77e-04   1.04e-03   1.20e-03   4.93e-04   1.24e-03   1.62e-03   1.14e-03   7.41e-04   1.68e-04   3.15e-04   2.37e-03   1.24e-04   7.39e-04   6.79e-04   1.24e-04   1.24e-04   1.24e-04   1.09e-03   5.05e-04   1.50e-03   6.95e-04   1.08e-04   1.08e-04   2.16e-04   1.86e-03   1.16e-03   1.58e-04   3.80e-04   1.19e-03   1.41e-04   2.65e-04   1.41e-04   2.42e-03   2.23e-04   3.91e-04   2.57e-04   5.38e-04   8.84e-04   3.15e-04   6.76e-04   9.64e-04   2.02e-03   3.69e-04   2.36e-03   5.66e-04   6.67e-04   2.63e-04   1.46e-03   1.12e-03   3.91e-04   9.94e-04   3.03e-04   3.16e-04   3.00e-03   2.57e-04   2.57e-04   4.86e-04   1.15e-04   1.15e-04   1.97e-03   5.54e-04   2.56e-04   1.15e-04   4.33e-04   3.34e-04   4.24e-04   5.33e-04   1.34e-04   1.34e-04   0.00e+00   9.47e-04   0.00e+00   8.44e-04   2.25e-04   1.41e-04   1.41e-04   9.85e-04   2.75e-04   5.71e-04   1.14e-03   8.78e-04   1.58e-04   1.58e-04   4.15e-04   1.60e-03   0.00e+00   1.82e-04   1.24e-04   0.00e+00   5.14e-04   2.26e-03   1.05e-04   8.18e-04   1.05e-04   2.23e-04   1.34e-04   3.14e-03   4.62e-04   7.68e-04   2.57e-04   2.57e-04   1.65e-03   6.31e-04   3.99e-04   1.68e-04   1.68e-04   1.68e-04   3.37e-04   5.36e-04   6.02e-04   4.48e-04   1.41e-04   1.41e-04   3.61e-04   1.55e-03   4.39e-04   1.82e-04   1.82e-04   1.68e-04   1.68e-04   3.09e-04   1.41e-04   1.68e-04   1.68e-04   6.40e-04   2.23e-04   5.58e-04   2.53e-04   6.86e-04   1.19e-04   6.70e-03   7.21e-04   7.27e-04   6.40e-04   3.64e-04   6.06e-04   1.49e-04   1.49e-04   0.00e+00   5.15e-04   2.57e-04   1.41e-04   1.41e-04   1.41e-04   2.82e-04   7.26e-04   2.21e-04   1.41e-04   4.80e-04   3.64e-04   2.57e-04   2.57e-04   2.23e-04   2.23e-04   1.29e-03   6.67e-04   1.58e-04   1.58e-04   1.68e-04   3.03e-04   6.86e-04   0.00e+00   1.05e-04   1.19e-04   7.34e-04   3.76e-04   3.92e-04   2.57e-04   1.68e-04   5.85e-04   3.99e-04   8.85e-04   3.64e-04   2.18e-03   1.99e-04   2.94e-04   4.65e-04   6.64e-04   2.98e-04   5.37e-04   2.57e-04   2.57e-04   1.09e-03   1.68e-04   1.68e-04   6.62e-04   6.14e-04   1.68e-04   1.68e-04   1.28e-03   3.01e-04   1.68e-04   9.88e-04   3.15e-04   1.82e-04   1.82e-04   1.19e-04   1.19e-04   2.38e-04   2.88e-04   1.82e-03   2.57e-04   2.23e-04   1.82e-04   1.82e-04   1.41e-04   4.15e-04   2.57e-04   3.91e-04   2.57e-04   2.23e-04   1.34e-04   1.34e-04   1.34e-04   1.82e-04   1.82e-04   1.82e-04   1.01e-03   2.23e-04   7.45e-04   1.58e-04   2.23e-04   5.66e-04   0.00e+00   1.82e-04   1.44e-03   4.39e-04   1.82e-04   2.70e-04   4.31e-04   3.51e-04   2.97e-04   3.39e-04   1.99e-04   1.99e-04   1.82e-04   1.68e-04   1.19e-04   2.52e-04   3.57e-04   1.42e-03   1.24e-04   2.58e-04   2.72e-04   1.24e-04   1.19e-04   1.19e-04   2.34e-04   9.78e-04   7.76e-04   1.19e-04   1.19e-04   1.19e-04   1.71e-03   0.00e+00   2.34e-03   6.86e-03   2.23e-04   1.49e-04   2.75e-04   1.99e-04   0.00e+00   0.00e+00   1.34e-04   1.34e-04   5.98e-04   1.68e-04   3.30e-04   1.99e-04   1.85e-03   5.15e-04   5.65e-04   3.68e-04   2.34e-04   2.37e-04   1.58e-04   1.58e-04   4.15e-04   1.82e-04   1.82e-04   1.68e-04   1.68e-04   3.15e-04   3.15e-04   1.58e-04   3.81e-04   1.99e-04   1.99e-04   1.68e-04   1.68e-04   1.49e-04   0.00e+00   0.00e+00   3.15e-04   3.14e-04   1.15e-04   4.72e-04   0.00e+00   2.23e-04   3.80e-04   2.23e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.99e-04   5.14e-04   1.82e-04   5.21e-04   1.58e-04   5.68e-04   1.29e-04   2.09e-04   6.78e-04   9.42e-04   1.41e-04   1.41e-04   4.56e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   6.20e-04   6.14e-04   5.60e-04   1.99e-04   4.98e-04   2.23e-04   2.83e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   2.24e-03   3.11e-04   2.23e-04   2.97e-04   4.30e-04   4.58e-04   5.91e-04   5.83e-04   5.87e-04   1.41e-04   1.41e-04   1.41e-04   7.95e-04   2.23e-04   7.17e-04   1.68e-04   3.33e-04   1.15e-04   1.99e-04   3.50e-04   1.82e-04   1.82e-04   3.28e-04   3.07e-04   1.99e-04   1.99e-04   1.29e-04   1.49e-04   2.57e-04   1.23e-03   0.00e+00   1.58e-04   2.23e-04   1.37e-03   0.00e+00   2.72e-04   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   0.00e+00   1.82e-04   1.82e-04   1.82e-04   1.99e-04   1.41e-04   1.41e-04   1.41e-04   2.23e-04   1.58e-04   1.58e-04   3.57e-04   2.97e-04   2.97e-04   5.90e-04   1.82e-04   1.82e-04   8.12e-04   0.00e+00   3.81e-04   1.68e-04   1.68e-04   0.00e+00   2.57e-04   5.72e-04   1.68e-04   9.48e-04   5.53e-04   3.09e-04   3.17e-04   1.68e-04   0.00e+00   0.00e+00   0.00e+00   1.41e-04   1.41e-04   1.41e-04   1.41e-04   2.23e-04   1.99e-04   3.15e-04   1.58e-04   6.67e-04   1.49e-04   2.77e-04   1.24e-03   1.24e-04   4.26e-04   5.49e-04   2.23e-04   2.23e-04   0.00e+00   8.48e-04   1.41e-04   1.41e-04   1.41e-04   1.82e-04   7.20e-04   1.58e-04   1.58e-04   1.58e-04   2.60e-04   1.34e-04   1.34e-04   1.34e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.82e-04   3.57e-04   4.22e-04   4.84e-04   5.85e-04   4.57e-04   2.23e-04   1.41e-04   1.41e-04   1.41e-04   1.58e-04   1.02e-03   3.80e-04   2.23e-04   2.23e-04   3.15e-04   5.72e-04   3.65e-04   1.05e-04   4.56e-04   4.57e-04   4.57e-04   1.24e-04   4.64e-04   1.24e-04   3.46e-04   3.39e-04   4.60e-04   1.06e-03   1.19e-04   5.18e-04   1.16e-03   4.96e-04   1.68e-04   2.92e-04   1.68e-04   2.70e-04   5.34e-04   1.49e-04   1.49e-04   6.53e-04   3.50e-04   1.68e-04   1.68e-04   1.68e-04   9.06e-04   2.81e-04   6.62e-04   1.58e-04   1.58e-04   2.77e-04   3.06e-04   3.17e-04   5.72e-04   9.48e-04   1.58e-04   1.58e-04   1.58e-04   3.78e-04   1.34e-04   1.34e-04   6.85e-04   6.65e-04   1.29e-04   1.34e-04   5.92e-04   3.23e-04   2.52e-04   3.57e-04   3.23e-04   1.24e-04   1.24e-04   3.23e-04   3.52e-04   1.24e-04   1.24e-04   2.92e-04   1.58e-04   1.58e-04   5.51e-04   1.41e-04   5.24e-04   1.41e-04   3.37e-04   6.14e-04   1.68e-04   1.49e-04   1.17e-03   1.29e-04   1.29e-04   1.29e-04   3.02e-04   3.06e-04   9.85e-04   1.82e-04   1.47e-03   1.41e-04   3.30e-04   1.68e-04   2.97e-04   3.75e-04   2.81e-04   1.99e-04   3.23e-04   1.99e-04   2.23e-04   5.71e-04   1.49e-04   1.68e-04   1.34e-04   1.34e-04   1.34e-04   4.65e-04   2.17e-04   4.55e-04   2.23e-04   3.42e-04   2.31e-04   1.41e-04   5.72e-04   4.57e-04   1.29e-04   1.29e-04   2.77e-04   1.29e-04   1.29e-04   1.99e-04   9.28e-04   1.99e-04   1.24e-04   1.24e-04   2.38e-04   1.19e-04   6.72e-04   2.30e-04   1.15e-04   1.15e-04   3.28e-04   3.28e-04   1.29e-04   1.29e-04   1.29e-04   3.28e-04   1.29e-04   1.68e-04   1.09e-03   8.01e-04   2.57e-04   1.41e-04   1.41e-04   8.21e-04   1.24e-04   5.46e-04   1.99e-04   1.55e-04   7.76e-05   7.76e-05   1.34e-04   1.34e-04   2.57e-04   2.57e-04   3.60e-04   6.75e-04   1.68e-04   1.68e-04   4.15e-04   1.68e-04   1.68e-04   2.57e-04   1.82e-04   3.16e-04   1.82e-04   2.17e-04   3.06e-04   1.49e-04   1.99e-04   3.15e-04   1.99e-04   1.99e-04   1.68e-04   4.26e-04   1.68e-04   1.71e-03   1.68e-04   1.08e-04   1.88e-04   1.91e-03   1.82e-04   1.82e-04   1.82e-04   1.29e-04   2.62e-04   1.82e-04   1.82e-04   2.23e-04   1.29e-04   1.29e-04   3.15e-04   2.89e-04   2.23e-03   1.19e-04   1.49e-04   1.49e-04   1.49e-04   5.12e-04   1.49e-04   4.28e-04   1.49e-04   1.41e-04   2.23e-04   1.68e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   2.32e-04   1.58e-04   4.57e-04   3.48e-04   1.68e-04   1.68e-04   7.61e-04   6.77e-04   1.99e-04   2.57e-04   1.24e-04   1.68e-04   8.31e-04   7.93e-04   1.58e-04   1.99e-04   1.82e-04   1.82e-04   3.50e-04   3.09e-04   1.34e-04   1.34e-04   1.34e-04   2.53e-04   1.34e-04   1.34e-04   1.68e-04   1.58e-04   2.23e-04   2.23e-04   1.58e-04   1.58e-04   1.58e-04   3.86e-04   7.16e-04   9.72e-05   6.04e-04   3.09e-04   8.30e-04   1.49e-04   2.80e-04   2.93e-04   1.11e-04   1.82e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   4.87e-04   1.83e-04   2.30e-04   2.30e-04   1.82e-04   3.81e-04   2.18e-03   1.49e-04   1.49e-04   1.49e-04   1.49e-04   3.15e-04   5.51e-04   1.82e-04   2.23e-04   2.57e-04   5.83e-04   2.57e-04   2.57e-04   1.41e-04   1.76e-03   1.41e-04   2.57e-04   1.29e-04   1.29e-04   3.37e-04   2.57e-04   8.41e-04   5.62e-04   1.68e-04   2.64e-04   3.15e-04   1.99e-04   1.99e-04   2.81e-04   1.99e-04   5.06e-04   2.57e-04   6.79e-04   1.68e-04   2.23e-04   1.82e-04   1.11e-04   1.11e-04   2.26e-04   1.58e-04   4.50e-04   1.58e-04   2.23e-04   1.58e-04   2.92e-04   1.82e-04   1.24e-04   2.57e-04   2.23e-04   5.25e-04   5.87e-04   3.64e-04   8.81e-04   1.82e-04   1.99e-04   1.99e-04   1.99e-04   2.23e-04   4.46e-04   1.82e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   2.88e-04   3.02e-04   1.99e-04   1.99e-04   1.07e-03   4.44e-04   1.82e-04   1.82e-04   3.58e-04   1.58e-04   1.58e-04   1.29e-04   9.00e-04   2.09e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   3.65e-04   2.97e-04   1.29e-04   1.34e-04   1.68e-04   1.68e-04   1.68e-04   1.41e-04   1.41e-04   1.41e-04   4.26e-04   2.57e-04   3.15e-04   1.58e-04   1.82e-04   6.13e-04   1.11e-04   1.11e-04   2.60e-04   1.49e-04   2.57e-04   1.11e-04   5.38e-04   1.24e-04   1.41e-04   1.41e-04   1.58e-04   1.49e-04   4.06e-04   3.65e-04   1.68e-04   1.68e-04   1.68e-04   1.34e-04   1.34e-04   1.82e-04   4.33e-04   2.23e-04   2.73e-04   1.58e-04   3.39e-04   3.42e-04   6.84e-04   2.60e-04   1.19e-04   1.29e-04   1.68e-04   1.68e-04   1.68e-04   2.57e-04   9.50e-05   9.50e-05   9.50e-05   9.50e-05   1.19e-04   2.48e-04   9.22e-04   1.82e-04   1.82e-04   1.99e-04   4.54e-04   1.58e-04   1.68e-04   1.29e-04   1.29e-04   1.29e-04   1.02e-04   1.02e-04   1.02e-04   1.99e-04   3.48e-03   2.61e-02   1.41e-04   1.22e-03   3.39e-03   2.75e-04   1.82e-04   1.82e-04   2.57e-04   1.58e-04   2.23e-04   6.57e-04   1.24e-04   2.35e-04   3.77e-04   2.07e-02   3.46e-04   1.24e-04   1.30e-03   5.12e-03   1.24e-04   4.27e-04   7.90e-04   4.96e-04   1.49e-04   3.72e-04   1.68e-04   1.68e-04   8.29e-04   1.30e-03   1.58e-04   2.86e-04   1.34e-04   8.20e-04   1.68e-04   5.54e-04   1.24e-04   2.92e-04   1.24e-04   1.06e-03   1.82e-04   1.24e-04   6.05e-03   3.02e-04   1.99e-04   3.15e-04   1.68e-04   1.68e-04   6.67e-04   2.35e-04   1.24e-04   5.96e-04   2.02e-04   9.72e-05   9.72e-05   9.72e-05   1.02e-03   3.57e-04   1.99e-04   1.13e-03   2.92e-03   5.94e-04   2.23e-04   5.24e-04   7.27e-04   1.68e-04   1.68e-04   7.57e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   2.23e-04   1.99e-04   1.99e-04   5.09e-04   1.99e-04   3.29e-03   3.81e-04   8.51e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   1.08e-04   2.13e-03   3.98e-04   1.08e-04   0.00e+00   1.49e-04   0.00e+00   4.65e-04   1.68e-04   1.99e-04   1.99e-04   2.23e-04   1.68e-04   1.68e-04   4.11e-04   3.39e-04   1.49e-04   1.49e-04   1.68e-04   1.68e-04   3.03e-04   6.86e-04   4.35e-04   1.49e-04   1.99e-04   1.99e-04   1.24e-04   1.24e-04   2.47e-04   2.29e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   3.02e-04   1.68e-04   3.16e-04   1.19e-04   2.23e-04   2.23e-04   1.00e-03   2.98e-04   1.05e-03   1.49e-04   1.49e-04   7.38e-04   6.34e-04   1.49e-04   2.60e-04   2.68e-04   1.49e-04   1.19e-04   3.79e-04   1.02e-04   1.02e-04   8.41e-04   1.02e-04   1.02e-04   1.11e-04   2.80e-04   1.11e-04   1.11e-04   1.11e-04   1.58e-04   7.49e-04   3.37e-04   5.10e-04   2.46e-04   1.34e-04   3.04e-04   1.99e-04   7.24e-04   7.24e-04   3.86e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   4.09e-04   2.57e-04   1.19e-04   3.42e-04   1.19e-04   6.71e-04   1.58e-04   6.88e-04   1.58e-04   3.11e-04   5.16e-04   7.93e-04   9.96e-05   3.76e-04   8.42e-05   8.42e-05   3.10e-04   8.42e-05   8.42e-05   8.42e-05   8.42e-05   4.09e-04   8.42e-05   7.21e-04   1.58e-04   2.50e-04   7.30e-04   2.81e-04   1.58e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.41e-04   1.41e-04   3.40e-04   3.39e-04   1.08e-04   2.01e-04   3.28e-04   2.52e-04   0.00e+00   0.00e+00   0.00e+00   0.00e+00   4.07e-04   2.58e-04   1.24e-04   1.24e-04   7.84e-04   3.15e-04   4.79e-04   3.81e-04   6.82e-04   1.15e-04   8.66e-04   1.15e-04   1.15e-04   2.49e-04   9.72e-05   2.46e-04   9.72e-05   9.72e-05   1.49e-04   1.49e-04   2.83e-04   1.49e-04   4.69e-04   2.23e-04   2.23e-04   3.92e-04   1.05e-04   1.05e-04   1.03e-03   8.01e-04   5.98e-04   8.74e-05   8.74e-05   8.74e-05   8.74e-05   8.74e-05   8.74e-05   4.12e-04   1.58e-04   1.41e-04   1.41e-04   4.88e-04   1.11e-04   1.11e-04   1.11e-04   1.99e-04   2.60e-04   2.23e-04   1.11e-04   2.84e-04   3.99e-04   1.58e-04   1.58e-04   1.58e-04   6.14e-04   7.95e-04   1.68e-04   1.68e-04   7.99e-04   1.68e-04   1.68e-04   3.03e-04   2.47e-04   7.56e-04   1.34e-04   2.75e-04   1.34e-04   1.34e-04   3.81e-04   5.43e-04   3.57e-04   2.99e-04   1.82e-04   1.99e-04   3.57e-04   1.99e-04   2.57e-04   1.68e-04   1.41e-04   1.41e-04   6.15e-04   1.41e-04   1.41e-04   6.73e-04   1.19e-04   3.06e-04   1.68e-04   1.68e-04   7.59e-04   1.68e-04   1.68e-04   1.41e-04   1.41e-04   1.29e-04   7.08e-04   8.42e-05   8.42e-05   8.42e-05   8.42e-05   8.42e-05   3.71e-04   8.42e-05   8.42e-05   8.42e-05   8.42e-05   2.23e-04   1.19e-04   1.19e-04   2.43e-04   1.19e-04   2.60e-04   1.19e-04   1.19e-04   1.41e-04   1.58e-04   1.99e-04   1.82e-04   2.65e-04   2.23e-04   6.64e-04   1.34e-04   3.39e-04   1.58e-04   9.50e-05   9.50e-05   1.95e-04   9.50e-05   2.53e-04   9.50e-05   9.50e-05   9.50e-05   9.50e-05   3.39e-04   7.14e-05   1.71e-04   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   7.14e-05   2.81e-04   7.14e-05   7.14e-05   7.14e-05   2.57e-04   2.01e-04   1.78e-04   8.91e-05   8.91e-05   3.23e-04   8.91e-05   3.81e-04   3.68e-04   1.68e-04   1.11e-04   1.11e-04   1.11e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.58e-04   1.58e-04   1.58e-04   1.58e-04   1.58e-04   2.65e-04   1.82e-04   1.99e-04   1.99e-04   1.99e-04   1.99e-04   1.99e-04   3.06e-03   1.82e-04   3.81e-04   2.57e-04   3.81e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   3.69e-04   1.11e-04   1.11e-04   6.80e-05   2.04e-04   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   4.89e-04   6.80e-05   6.80e-05   2.50e-04   6.80e-05   2.91e-04   6.80e-05   9.50e-05   9.50e-05   1.90e-04   9.50e-05   9.50e-05   9.50e-05   9.50e-05   9.50e-05   6.60e-04   2.58e-04   8.91e-05   8.91e-05   8.91e-05   8.91e-05   8.91e-05   8.91e-05   8.91e-05   1.68e-04   4.91e-04   2.23e-04   1.29e-04   1.29e-04   3.74e-04   3.37e-04   2.23e-04   4.79e-04   3.78e-03   1.82e-04   0.00e+00   2.57e-04   1.19e-04   1.19e-04   4.30e-04   1.34e-04   1.68e-04   4.39e-04   1.29e-04   2.97e-04   1.29e-04   1.29e-04   1.41e-04   2.56e-04   3.30e-04   1.24e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   3.39e-04   1.82e-04   1.49e-04   1.49e-04   3.06e-04   1.49e-04   1.49e-04   1.99e-04   1.49e-04   2.77e-04   1.82e-04   2.72e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   0.00e+00   1.02e-04   1.68e-04   1.68e-04   1.68e-04   4.73e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   6.71e-04   1.08e-04   5.67e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.49e-04   1.34e-04   1.34e-04   1.11e-04   2.16e-04   1.99e-04   1.99e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.08e-04   1.08e-04   3.98e-04   1.08e-04   1.08e-04   1.82e-04   2.82e-04   1.82e-04   1.82e-04   1.08e-04   1.08e-04   2.27e-04   3.20e-04   2.66e-04   1.08e-04   1.08e-04   1.11e-04   1.11e-04   5.72e-04   1.24e-04   1.24e-04   1.41e-04   4.69e-04   1.29e-04   1.29e-04   1.29e-04   1.58e-04   1.58e-04   6.08e-04   1.99e-04   0.00e+00   0.00e+00   6.80e-05   6.80e-05   6.80e-05   6.80e-05   2.02e-04   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   1.36e-04   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   6.80e-05   8.42e-05   2.03e-04   2.33e-04   2.19e-04   3.06e-04   8.42e-05   8.42e-05   8.42e-05   8.42e-05   9.96e-05   3.62e-04   9.96e-05   1.41e-04   4.57e-04   1.41e-04   1.29e-04   2.64e-04   2.64e-04   1.49e-04   1.58e-04   1.58e-04   1.58e-04   1.15e-04   1.15e-04   0.00e+00   1.05e-04   2.46e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   9.29e-05   2.50e-04   9.29e-05   9.29e-05   9.29e-05   2.04e-04   9.29e-05   0.00e+00   0.00e+00   1.29e-04   1.29e-04   1.41e-04   1.41e-04   1.41e-04   3.15e-04   1.82e-04   3.50e-04   3.92e-04   1.34e-04   1.34e-04   2.46e-04   2.46e-04   7.20e-04   1.11e-04   1.11e-04   2.35e-04   1.99e-04   1.99e-04   1.29e-04   1.29e-04   6.99e-04   1.58e-04   1.58e-04   1.58e-04   5.19e-04   2.11e-04   9.96e-05   9.96e-05   2.11e-04   9.96e-05   9.96e-05   9.96e-05   2.11e-04   2.82e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   2.26e-04   1.11e-04   3.81e-04   1.82e-04   3.42e-04   1.19e-04   1.19e-04   1.19e-04   2.31e-04   1.19e-04   1.19e-04   1.19e-04   2.64e-04   3.57e-04   3.17e-04   4.06e-04   2.69e-04   8.74e-05   1.75e-04   8.74e-05   8.74e-05   8.74e-05   3.10e-04   1.34e-04   2.57e-04   4.84e-04   3.37e-04   8.42e-05   8.42e-05   8.42e-05   8.42e-05   8.42e-05   2.57e-04   3.09e-04   1.05e-04   1.05e-04   1.05e-04   3.13e-04   1.05e-04   2.57e-04   2.57e-04   2.57e-04   2.57e-04   1.34e-04   2.53e-04   1.34e-04   1.68e-04   1.68e-04   7.84e-04   1.49e-04   3.68e-04   1.99e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   3.23e-04   1.99e-04   4.39e-04   2.48e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.49e-04   3.92e-04   1.49e-04   1.49e-04   1.49e-04   1.68e-04   3.64e-04   1.82e-04   1.58e-04   1.34e-04   2.60e-04   2.60e-04   0.00e+00   9.29e-05   9.29e-05   9.29e-05   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.34e-04   1.34e-04   2.92e-04   1.34e-04   1.34e-04   1.34e-04   1.99e-04   2.57e-04   2.23e-04   2.23e-04   1.99e-04   1.11e-04   1.11e-04   1.11e-04   1.99e-04   4.91e-04   3.09e-04   1.41e-04   1.41e-04   1.41e-04   2.89e-04   1.41e-04   3.15e-04   9.29e-05   7.39e-04   9.29e-05   9.29e-05   4.67e-04   9.29e-05   9.29e-05   9.29e-05   9.29e-05   1.49e-04   1.49e-04   1.49e-04   5.44e-04   1.49e-04   3.06e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   6.40e-04   4.07e-04   1.34e-04   1.41e-04   1.68e-04   2.08e-04   1.86e-04   2.04e-04   9.29e-05   7.21e-04   6.53e-04   2.98e-04   1.58e-04   4.75e-04   1.58e-04   1.58e-04   3.80e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   3.40e-04   1.41e-04   1.41e-04   3.09e-04   1.34e-04   1.34e-04   3.87e-04   1.72e-04   8.58e-05   8.58e-05   8.58e-05   8.58e-05   1.82e-04   1.82e-04   1.15e-04   1.15e-04   1.15e-04   1.15e-04   1.15e-04   1.11e-04   2.35e-04   1.11e-04   1.11e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   3.99e-04   1.99e-04   1.99e-04   1.82e-04   9.72e-05   9.72e-05   9.72e-05   9.72e-05   1.51e-03   2.26e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   1.05e-04   1.05e-04   1.01e-03   1.05e-04   3.14e-04   2.57e-04   2.57e-04   1.20e-03   1.11e-04   2.40e-04   1.11e-04   1.11e-04   8.42e-05   1.92e-04   8.42e-05   8.42e-05   1.99e-04   1.19e-04   1.19e-04   2.43e-04   1.19e-04   2.57e-04   1.11e-04   2.77e-04   4.45e-04   1.29e-04   2.57e-04   2.57e-04   1.12e-03   1.49e-04   1.49e-04   1.99e-04   1.99e-04   1.99e-04   2.23e-04   2.23e-04   1.02e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   2.83e-04   5.33e-04   9.72e-05   9.72e-05   9.72e-05   9.72e-05   9.72e-05   4.46e-04   1.99e-04   2.29e-04   1.34e-04   1.34e-04   1.34e-04   3.03e-04   1.34e-04   1.82e-04   1.82e-04   3.34e-04   3.15e-04   3.37e-04   1.15e-04   1.15e-04   2.23e-04   2.23e-04   2.23e-04   1.82e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.49e-04   1.49e-04   1.49e-04   2.40e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.58e-04   5.47e-04   1.99e-04   3.26e-04   3.15e-04   4.58e-04   1.68e-04   1.34e-04   1.82e-04   1.82e-04   1.19e-04   1.19e-04   1.58e-04   4.47e-04   7.21e-04   2.64e-04   1.29e-04   1.29e-04   1.99e-04   1.41e-04   1.49e-04   3.86e-04   1.49e-04   1.58e-04   1.58e-04   1.99e-04   1.99e-04   8.01e-04   1.58e-04   1.58e-04   5.56e-04   1.24e-04   3.81e-04   1.24e-04   2.51e-04   1.49e-04   1.49e-04   9.29e-05   9.29e-05   9.29e-05   9.29e-05   9.29e-05   9.29e-05   2.60e-04   1.11e-04   1.11e-04   1.11e-04   1.11e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   1.34e-04   1.34e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.99e-04   1.49e-04   1.49e-04   1.49e-04   1.99e-04   2.57e-04   1.82e-04   1.82e-04   8.81e-04   3.14e-04   1.99e-04   2.57e-04   2.57e-04   3.15e-04   1.24e-04   3.96e-04   1.24e-04   1.15e-04   1.15e-04   1.58e-04   2.23e-04   2.23e-04   1.82e-04   1.41e-04   1.41e-04   2.23e-04   3.86e-04   2.82e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   1.58e-04   1.58e-04   1.58e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   4.45e-04   1.58e-04   1.58e-04   1.68e-04   1.68e-04   9.50e-05   9.50e-05   1.29e-04   1.29e-04   7.41e-04   2.57e-04   1.99e-04   2.23e-04   1.58e-04   1.58e-04   2.57e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   1.82e-04   2.57e-04   2.57e-04   2.57e-04   3.86e-04   1.49e-04   1.49e-04   2.23e-04   2.23e-04   3.81e-04   2.23e-04   1.68e-04   1.68e-04   3.91e-04   2.34e-04   8.91e-05   8.91e-05   8.91e-05   8.91e-05   8.91e-05   1.15e-04   1.15e-04   1.99e-04   1.99e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   3.09e-04   1.58e-04   1.68e-04   1.68e-04   1.41e-04   1.41e-04   1.99e-04   1.41e-04   1.41e-04   1.58e-04   1.58e-04   8.27e-04   1.68e-04   1.29e-04   1.29e-04   1.99e-04   2.23e-04   1.08e-04   1.08e-04   1.49e-04   1.49e-04   1.49e-04   1.24e-04   1.82e-04   1.82e-04   1.49e-04   1.49e-04   1.49e-04   4.01e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   2.57e-04   1.41e-04   4.46e-04   1.68e-04   1.68e-04   1.68e-04   1.49e-04   2.29e-04   1.49e-04   1.19e-04   1.19e-04   1.82e-04   1.68e-04   8.00e-05   8.00e-05   1.60e-04   8.00e-05   8.00e-05   1.95e-04   9.50e-05   1.24e-04   1.24e-04   1.34e-04   1.34e-04   1.34e-04   3.72e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.08e-04   1.15e-04   2.44e-04   1.15e-04   1.15e-04   1.41e-04   1.68e-04   1.68e-04   1.68e-04   1.68e-04   1.49e-04   2.23e-04   1.68e-04   2.23e-04   1.58e-04   1.68e-04   1.68e-04   2.92e-04   1.58e-04   1.58e-04   1.58e-04   3.11e-04   1.99e-04   1.99e-04   1.99e-04   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   1.95e-04   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   2.23e-04   3.50e-04   1.82e-04   1.68e-04   2.88e-04   1.29e-04   2.70e-04   2.23e-04   1.82e-04   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   8.00e-05   2.57e-04   2.73e-04   1.41e-04   0.00e+00   0.00e+00   1.19e-04   1.19e-04   1.19e-04   1.68e-04   1.68e-04   1.99e-04   1.29e-04   1.29e-04   1.29e-04   1.68e-04   1.82e-04   3.11e-04   1.15e-04   1.15e-04   2.23e-04   2.23e-04   1.41e-04   1.15e-04   1.15e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   1.24e-04   2.92e-04   1.24e-04   3.76e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.99e-04   1.99e-04   1.99e-04   1.68e-04   1.24e-04   1.24e-04   1.24e-04   3.99e-04   4.01e-04   1.41e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   3.11e-04   1.29e-04   1.05e-04   1.05e-04   1.05e-04   1.05e-04   1.11e-04   3.15e-04   1.24e-04   1.24e-04   1.82e-04   1.82e-04   1.58e-04   1.58e-04   1.19e-04   1.19e-04   1.19e-04   1.19e-04   1.15e-04   1.15e-04   1.15e-04   1.15e-04   1.82e-04   1.41e-04   1.29e-04   1.29e-04   1.99e-04   1.99e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   1.49e-04   1.49e-04   1.82e-04   1.15e-04   1.29e-04   1.29e-04   1.29e-04   1.29e-04   2.57e-04   1.49e-04   1.49e-04   1.49e-04   2.57e-04   2.57e-04   0.00e+00   0.00e+00   0.00e+00   0.00e+00   1.19e-04   1.82e-04   1.99e-04   2.23e-04   1.49e-04   1.49e-04   1.72e-04   8.58e-05   3.26e-04   3.80e-04   2.23e-04   2.57e-04   2.57e-04   1.68e-04   1.68e-04   1.68e-04   3.15e-04   1.49e-04   1.49e-04   1.49e-04   2.23e-04   1.41e-04   1.41e-04   1.41e-04   1.41e-04   2.70e-04   1.24e-04   1.24e-04   1.82e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.02e-04   1.34e-04   1.34e-04   1.34e-04   1.34e-04   1.41e-04   1.41e-04   1.41e-04   1.99e-04   8.42e-05   8.42e-05   1.68e-04   8.42e-05   8.42e-05   8.42e-05   1.49e-04   1.49e-04   1.68e-04   3.91e-04   4.26e-04   1.49e-04   1.49e-04   1.49e-04   1.68e-04   1.58e-04   2.57e-04   1.24e-04   1.24e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   1.49e-04   2.23e-04   1.99e-04   1.99e-04   1.68e-04   1.68e-04   1.29e-04   1.82e-04   1.82e-04   1.82e-04   2.23e-04   2.57e-04]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print centers[0]\n",
    "#k_means(test_pointsz, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02  0.01  0.    0.01  0.51  0.    0.    0.    0.    0.    0.01  0.02  0.    0.    0.    0.01  0.    0.    0.01  0.01  0.    0.    0.    0.06  0.    0.08  0.    0.01  0.    0.    0.    0.01  0.    0.    0.01  0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.02  0.    0.01  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.01  0.03  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.    0.01  0.    0.    0.    0.    0.01  0.01  0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.02  0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.03  0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.01  0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.03  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "print centers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5591666666666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(train_label, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy is 0.56, only slightly better than random guess(0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train_feat, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80666666666666664"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_feat, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classification accuracy is 0.807."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,  42],\n",
       "       [ 74, 226]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_label, model.predict(test_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauti\n",
      "fantast\n",
      "best\n",
      "good\n",
      "amaz\n",
      "delici\n",
      "nice\n",
      "excel\n",
      "love\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "coef = model.coef_\n",
    "for pos in coef.argsort()[0,-10:]:\n",
    "    print word_list[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printing result we can see, strong positive emotional words play a great role in determining labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_dict, n_gram_list = {}, []\n",
    "\n",
    "def n_gram_feature(data, n = 2):\n",
    "    pos = 0\n",
    "    \n",
    "    for sentence in data['sentence']:\n",
    "        tokens = sentence.split(' ')\n",
    "        for index in range(len(tokens) - n + 1):\n",
    "            now_gram = ' '.join(tokens[index : (index + n)])\n",
    "            if not now_gram in n_gram_dict:\n",
    "                n_gram_dict[now_gram] = pos\n",
    "                n_gram_list.append(now_gram)\n",
    "                pos += 1\n",
    "\n",
    "n_gram_feature(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_n_gram_feature(data, n = 2):\n",
    "    rows, cols = len(data['sentence']), len(n_gram_dict)\n",
    "    res = np.zeros((rows,cols), dtype = np.int)\n",
    "    \n",
    "    for index, sentence in enumerate(data['sentence']):\n",
    "        tokens = sentence.split(' ')\n",
    "        for token_idx in range(len(tokens) - n + 1):\n",
    "            now_gram = ' '.join(tokens[token_idx : (token_idx + n)])\n",
    "            if now_gram in n_gram_dict:\n",
    "                res[index][n_gram_dict[now_gram]] += 1\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_2_gram_feat = build_n_gram_feature(train)\n",
    "test_2_gram_feat = build_n_gram_feature(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_2_gram_feat = normalize(train_2_gram_feat)\n",
    "test_2_gram_feat = normalize(test_2_gram_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training set clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n"
     ]
    }
   ],
   "source": [
    "i1,i2 = np.random.choice(train_2_gram_feat.shape[0],2, replace=False)\n",
    "start_points = train_2_gram_feat[[i1,i2]]\n",
    "centers, labels = find_k_centroids(start_points, train_2_gram_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50291666666666668"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering accuracy is bad, almost same like random guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_2_gram_feat, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64166666666666672"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_2_gram_feat, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classification accuracy based on 2-gram is 0.642. Not as good as 1-gram, possible reason could be 2-gram is more sparse than 1-gram thus lack generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[266,  34],\n",
       "       [181, 119]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_label, model.predict(test_2_gram_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good price\n",
      "realli good\n",
      "great food\n",
      "easi use\n",
      "food good\n",
      "great product\n",
      "great phone\n",
      "one best\n",
      "highli recommend\n",
      "work great\n"
     ]
    }
   ],
   "source": [
    "coef = model.coef_\n",
    "for pos in coef.argsort()[0,-10:]:\n",
    "    print n_gram_list[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printing result we can see, 2-grams having strong positive emotional word can be a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pca(data, n = 10):\n",
    "    U, s, V = np.linalg.svd(data, full_matrices=True)\n",
    "    delta = np.diag(s[:n])\n",
    "\n",
    "    return np.dot(U[:,:n],delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test11 = np.array([[1.9,2.5,3.3], \n",
    "                   [2.2,3.1,4.8], \n",
    "                   [2.1,0.1,5.4]])\n",
    "#train_pca_10_feat = pca(train_feat, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def center(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    return data - mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pca_10_feat = pca(train_feat, 10)\n",
    "test_pca_10_feat = pca(test_feat, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n",
      "iter 10\n",
      "iter 11\n",
      "iter 12\n",
      "iter 13\n",
      "iter 14\n",
      "iter 15\n",
      "iter 16\n",
      "iter 17\n",
      "iter 18\n",
      "iter 19\n"
     ]
    }
   ],
   "source": [
    "def find_start_points(feat, n = 2):\n",
    "    i1,i2 = np.random.choice(feat.shape[0],2, replace=False)\n",
    "    start_points = feat[[i1,i2]]\n",
    "\n",
    "    return start_points\n",
    "\n",
    "start_points = find_start_points(train_pca_10_feat)\n",
    "centers, labels = find_k_centroids(start_points, train_pca_10_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.09, -0.05, -0.07, -0.04,  0.2 , -0.06,  0.01, -0.03, -0.12,  0.09]), array([ 0.08, -0.03, -0.05,  0.01,  0.  , -0.01, -0.  , -0.  ,  0.02, -0.01])]\n"
     ]
    }
   ],
   "source": [
    "print centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52541666666666664"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means result on data reducing  dimensionality to 10 only slightly better than random guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_pca_10_feat, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54166666666666663"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_pca_10_feat, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pca_50_feat = pca(train_feat, 50)\n",
    "test_pca_50_feat = pca(test_feat, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n"
     ]
    }
   ],
   "source": [
    "start_points = find_start_points(train_pca_50_feat)\n",
    "centers, labels = find_k_centroids(start_points, train_pca_50_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  4.65e-01,   2.12e-01,   8.69e-02,  -8.35e-02,  -2.55e-02,   1.77e-02,  -3.04e-02,  -2.47e-02,  -1.22e-02,   9.18e-03,  -8.76e-03,  -2.28e-03,   2.77e-03,  -7.11e-03,  -8.07e-03,  -9.17e-03,   4.17e-03,  -3.08e-03,   8.32e-04,  -1.30e-03,  -3.99e-03,  -1.40e-03,   4.30e-03,  -8.90e-03,  -1.66e-03,   5.20e-04,  -4.12e-04,   6.97e-04,  -2.09e-03,  -4.76e-04,  -7.15e-04,   2.16e-03,  -1.05e-03,   2.82e-03,   2.12e-03,   9.42e-04,   2.74e-03,   1.93e-03,  -1.51e-03,   1.32e-03,  -5.55e-04,   5.10e-04,   2.01e-03,   1.21e-03,   1.04e-03,   9.58e-04,   1.07e-03,  -1.05e-03,   1.57e-03,   1.08e-03]), array([  5.75e-02,  -4.48e-02,  -6.14e-02,   1.13e-02,   2.47e-02,  -2.15e-02,  -6.72e-04,  -2.12e-03,   4.88e-03,   2.14e-03,   1.61e-03,  -5.62e-03,   1.89e-03,   2.21e-03,   1.01e-03,  -7.19e-03,  -5.31e-03,   1.10e-02,   9.49e-04,  -5.20e-04,  -4.75e-03,   2.26e-03,  -4.77e-03,  -3.13e-03,   3.49e-03,  -6.55e-03,   9.33e-03,   4.13e-03,  -6.75e-04,  -1.21e-03,   3.94e-05,  -1.26e-03,  -4.75e-03,  -4.02e-03,  -3.85e-03,   2.64e-04,   9.82e-04,   4.11e-03,   1.10e-03,   9.86e-04,   5.41e-03,  -5.01e-03,   1.81e-03,   6.48e-03,  -6.69e-04,  -1.29e-03,  -1.49e-03,   7.21e-04,  -8.44e-04,  -2.57e-03])]\n"
     ]
    }
   ],
   "source": [
    "print centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44124999999999998"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accurancy is 0.56(1 - 0.44)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52666666666666662"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_pca_50_feat, train_label)\n",
    "model.score(test_pca_50_feat, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n"
     ]
    }
   ],
   "source": [
    "train_pca_100_feat = pca(train_feat, 100)\n",
    "test_pca_100_feat = pca(test_feat, 100)\n",
    "start_points = find_start_points(train_pca_100_feat)\n",
    "centers, labels = find_k_centroids(start_points, train_pca_100_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  2.47e-01,   7.28e-02,  -5.90e-02,   1.06e-01,  -7.06e-02,  -1.55e-01,   3.03e-01,   8.47e-02,   3.10e-02,  -3.29e-02,   7.00e-03,   2.19e-02,  -1.61e-03,   3.42e-02,   8.29e-03,   5.78e-02,   5.63e-03,  -2.36e-02,   9.74e-03,   4.76e-04,   3.80e-02,  -2.05e-02,  -3.40e-02,   8.93e-03,   1.25e-02,   9.39e-04,   1.91e-03,  -3.18e-03,   2.29e-03,   1.98e-03,  -2.10e-03,  -1.25e-04,  -1.16e-03,  -5.07e-05,  -8.95e-03,   8.17e-04,  -1.41e-03,   3.13e-04,  -7.16e-04,   1.66e-03,  -2.31e-03,  -2.43e-03,   5.13e-05,   1.91e-03,  -2.11e-03,   2.10e-04,  -2.68e-03,  -9.99e-04,  -2.68e-03,   5.46e-04,  -1.65e-03,   1.01e-03,  -2.84e-03,  -3.33e-04,  -1.15e-04,   6.60e-04,   6.45e-04,   2.57e-03,  -2.36e-04,   9.60e-04,  -4.16e-04,   1.00e-03,  -2.09e-03,  -7.83e-04,   1.35e-03,  -1.63e-03,  -4.84e-04,   9.65e-04,  -6.26e-04,  -5.52e-04,   3.47e-04,   9.44e-04,  -1.33e-03,  -2.77e-03,  -6.34e-05,   1.68e-03,  -1.29e-03,  -8.26e-04,  -2.43e-04,   1.48e-03,  -1.94e-03,  -7.28e-04,  -2.47e-04,   9.59e-04,  -7.11e-04,   4.26e-04,  -1.64e-04,  -1.15e-03,  -3.25e-03,   3.38e-04,   3.28e-04,   1.59e-03,   2.83e-03,  -1.94e-03,  -5.68e-04,  -1.50e-03,  -2.39e-04,   1.17e-03,  -1.35e-03,   3.96e-05]), array([  7.58e-02,  -3.32e-02,  -5.15e-02,   2.26e-04,   2.60e-02,  -1.24e-02,  -1.76e-02,  -7.90e-03,   2.44e-03,   4.33e-03,   6.40e-04,  -6.74e-03,   2.12e-03,   1.67e-05,   4.10e-05,  -1.05e-02,  -5.20e-03,   1.17e-02,   5.11e-04,  -6.22e-04,  -6.79e-03,   3.12e-03,  -2.72e-03,  -4.11e-03,   2.70e-03,  -6.44e-03,   9.03e-03,   4.26e-03,  -9.16e-04,  -1.32e-03,   9.30e-05,  -1.08e-03,  -4.67e-03,  -3.75e-03,  -3.20e-03,   2.82e-04,   1.22e-03,   4.15e-03,   1.01e-03,   9.75e-04,   5.39e-03,  -4.76e-03,   1.91e-03,   6.35e-03,  -4.82e-04,  -1.21e-03,  -1.26e-03,   6.86e-04,  -5.90e-04,  -2.47e-03,   2.28e-03,   6.86e-06,  -2.31e-03,   2.49e-03,  -7.25e-04,  -3.01e-03,  -2.59e-03,   1.16e-03,  -3.44e-04,   3.22e-03,   1.17e-03,  -4.82e-03,  -3.51e-03,  -2.53e-03,   1.59e-03,  -1.93e-04,   2.10e-03,  -2.04e-03,   2.12e-03,   9.44e-04,   4.94e-04,  -3.64e-04,  -1.41e-03,   6.60e-04,   2.70e-04,   4.73e-05,  -4.12e-04,   3.29e-03,   2.57e-03,  -1.07e-03,   3.51e-06,   7.89e-04,   9.32e-04,   1.48e-03,   2.20e-04,   2.06e-03,   2.52e-04,  -1.53e-03,  -1.01e-03,   1.80e-03,  -1.22e-03,  -1.37e-04,  -1.58e-03,   1.18e-03,  -2.37e-04,   1.24e-03,   2.15e-03,  -1.33e-05,  -1.89e-04,  -1.22e-03])]\n"
     ]
    }
   ],
   "source": [
    "print centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51\n"
     ]
    }
   ],
   "source": [
    "sc = accuracy_score(train_label, labels)\n",
    "print max(sc, 1 - sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j Algorithms comparison and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words representation features combined with using logistic regression achieves best performance. K-means result is around 50% whether using bag of words representation, 2-gram or pca for bag-of-words. The reason why pca for bag-of-words combined with logistic regression is not as good as raw bag-of-words features could be that pca requires centerized data, but the absolute number of each word could be the good indicator; and also, maybe 10,50,100 dimensions is relatively small compared to 3673 original dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
